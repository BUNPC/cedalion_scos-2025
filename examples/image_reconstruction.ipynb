{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cdf1b6",
   "metadata": {},
   "source": [
    "# Image Reconstruction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e869404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "import pyvista as pv\n",
    "#pv.set_jupyter_backend('html')\n",
    "pv.set_jupyter_backend('static')\n",
    "#pv.OFF_SCREEN=True\n",
    "\n",
    "import cedalion\n",
    "import cedalion.io\n",
    "import cedalion.dataclasses as cdc\n",
    "import cedalion.geometry.registration\n",
    "import cedalion.geometry.segmentation\n",
    "import cedalion.plots\n",
    "import cedalion.xrutils as xrutils\n",
    "import cedalion.imagereco.tissue_properties\n",
    "import cedalion.imagereco.forward_model as fw\n",
    "from cedalion.imagereco.solver import pseudo_inverse_stacked\n",
    "import cedalion.datasets\n",
    "from cedalion import units\n",
    "import pickle\n",
    "from gzip import GzipFile\n",
    "import matplotlib.pyplot as p\n",
    "import time\n",
    "\n",
    "xr.set_options(display_expand_data=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bab37",
   "metadata": {},
   "source": [
    "## Load a finger-tapping dataset \n",
    "\n",
    "For this demo we load an example finger-tapping recording through `cedalion.datasets.get_fingertapping`. The file contains a single NIRS element with one block of raw amplitude data. \n",
    "\n",
    "Alternatively, use `cedalion.io.read_snirf` to read elements from a snirf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1425f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = cedalion.datasets.get_fingertapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b9f72",
   "metadata": {},
   "source": [
    "The location of the probes is obtained from the snirf metadata (i.e. /nirs0/probe/)\n",
    "\n",
    "Note that units ('m') are adopted and the coordinate system is named 'digitized'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3d_meas = elements[0].geo3d\n",
    "display(geo3d_meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa371e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cedalion.plots.plot_montage3D(elements[0].data[0], geo3d_meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b84f2",
   "metadata": {},
   "source": [
    "The measurement list is a `pandas.DataFrame` that describes which source detector pairs form channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74070def",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_list = elements[0].measurement_lists[0]\n",
    "display(meas_list.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650bcb15",
   "metadata": {},
   "source": [
    "For this demo select 20 seconds after a trial starts at t=117s and transform raw amplitudes to optical density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_amp = elements[0].data[0]\n",
    "od = -np.log( raw_amp/ raw_amp.mean(\"time\"))\n",
    "od = od.sel(time=(od.time > 117) & (od.time < 137))\n",
    "display(od)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.figure(figsize=(12,2))\n",
    "for ch in [\"S1D1\", \"S1D2\", \"S1D3\"]:\n",
    "    for wl in od.wavelength.values:\n",
    "        p.plot(od.time, od.sel(channel=ch, wavelength=wl), label=f\"{ch} {wl} nm\")\n",
    "p.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a3766",
   "metadata": {},
   "source": [
    "## Load segmented MRI scan\n",
    "\n",
    "For this example use a segmentation of the Colin27 average brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_DATADIR, mask_files, landmarks_file = cedalion.datasets.get_colin27_segmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a996c",
   "metadata": {},
   "source": [
    "The segmentation masks are in individual niftii files. The dict `mask_files` maps mask filenames relative to `SEG_DATADIR` to short labels. These labels describe the tissue type of the mask. \n",
    "\n",
    "In principle the user is free to choose these labels. However, they are later used to lookup the tissue's optical properties. So they must be map to one of the tabulated tissue types (c.f. `cedalion.imagereco.tissue_properties.TISSUE_LABELS`).\n",
    "\n",
    "The variable `landmarks_file` holds the path to a file containing landmark positions in scanner space (RAS). This file can be created with Slicer3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ed30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SEG_DATADIR)\n",
    "display(mask_files)\n",
    "display(landmarks_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e77868",
   "metadata": {},
   "source": [
    "## Coordinate systems\n",
    "\n",
    "Up to now we have geometrical data from three different coordinate reference systems (CRS):\n",
    "\n",
    "- The optode positions are in one space `CRS='digitized'` and the coordinates are in meter. In our example the origin is at the head center and y-axis pointing in the superior direction. Other digitization tools can use other units or coordinate systems.\n",
    "- The segmentation masks are in voxel space (`CRS='ijk'`) in which the voxel edges are aligned with the coordinate axes. Each voxel has unit edge length, i.e. coordinates are dimensionless. \n",
    "  Axis-aligned grids are computationally efficient, which is why the photon simulation code (MCX) uses this coordinate system.\n",
    "- The voxel space (`CRS='ijk'`) is related to scanner space (`CRS='ras'` or `CRS='aligned'`) in which coordinates have physical units and coordinate axes point to the (r)ight, (a)nterior and s(uperior) directions. The relation between both spaces is given through an affine transformation (e.g. `t_ijk2ras`). When loading the segmentation masks in Slicer3D this transformation is automatically applied. Hence, the picked landmark coordinates are exported in RAS space.\n",
    "\n",
    "  The niftii file provides a string label for the scanner space. In this example the RAS space is called 'aligned' because the masks are aligned to another MRI scan.\n",
    "\n",
    "\n",
    "To avoid confusion between these different coordinate systems, `cedalion` tries to be explicit about which CRS a given point cloud or surface is in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564a12b",
   "metadata": {},
   "source": [
    "## The TwoSurfaceHeadModel\n",
    "\n",
    "The photon propagation considers the complete MRI scan, in which each voxel is attributed to one tissue type with its respective optical properties. However, the image reconstruction does not intend to reconstruct absorption changes in each voxel. The inverse problem is simplified, by considering only two surfaces (scalp and brain) and reconstruct only absorption changes in voxels close to these surfaces.\n",
    "\n",
    "The class `cedalion.imagereco.forward_model.TwoSurfaceHeadModel` groups together the segmentation mask, landmark positions and affine transformations as well as the scalp and brain surfaces. The brain surface is calculated by grouping together white and gray matter masks. The scalp surface encloses the whole head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = fw.TwoSurfaceHeadModel.from_segmentation(\n",
    "    segmentation_dir=SEG_DATADIR,\n",
    "    mask_files = mask_files,\n",
    "    landmarks_ras_file=landmarks_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.segmentation_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa792474",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2372eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decf01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.scalp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b36473",
   "metadata": {},
   "source": [
    "`TwoSurfaceHeadModel.from_segmentation` converts everything into voxel space (`CRS='ijk'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4193ba",
   "metadata": {},
   "source": [
    "The transformation matrix to translate from voxel to scanner space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2364a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "head.t_ijk2ras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421a81c",
   "metadata": {},
   "source": [
    "Changing between coordinate systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_ras = head.apply_transform(head.t_ijk2ras)\n",
    "display(head_ras.crs)\n",
    "display(head_ras.brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4fe577",
   "metadata": {},
   "source": [
    "## Optode Registration\n",
    "The optode coordinates from the recording must be aligned with the scalp surface. Currently, `cedaĺion` offers a simple registration method, which finds an affine transformation (scaling, rotating, translating) that matches the landmark positions of the head model and their digitized counter parts. Afterwards, optodes are snapped to the nearest vertex on the scalp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a47069",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3d_snapped_ijk = head.align_and_snap_to_scalp(geo3d_meas)\n",
    "display(geo3d_snapped_ijk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea835eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = pv.Plotter()\n",
    "cedalion.plots.plot_surface(plt, head.brain, color=\"w\")\n",
    "cedalion.plots.plot_surface(plt, head.scalp, opacity=.1)\n",
    "cedalion.plots.plot_labeled_points(plt, geo3d_snapped_ijk)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90953f24",
   "metadata": {},
   "source": [
    "## Simulate light propagation in tissue with MCX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9857b",
   "metadata": {},
   "source": [
    "`cedalion.imagereco.forward_model.ForwardModel` is a wrapper around pmcx. Using the data in the head model it prepares the inputs for pmcx and offers functionality to calculate the sensitivty matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84957975",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwm = cedalion.imagereco.forward_model.ForwardModel(head, geo3d_snapped_ijk, meas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25934ae7",
   "metadata": {},
   "source": [
    "### Run the simulation\n",
    "\n",
    "The `compute_fluence` method puts a simulated light source at each optode positions and calulcates the fluence in each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6757c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RUN_MCX = False\n",
    "\n",
    "if RUN_MCX:\n",
    "    fluence_all, fluence_at_optodes = fwm.compute_fluence()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    fluence_all, fluence_at_optodes = cedalion.datasets.get_imagereco_example_fluence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70951a",
   "metadata": {},
   "source": [
    "The photon simulation yields the fluence in each voxel fo each wavelength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluence_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278521be",
   "metadata": {},
   "source": [
    "Also, for a each combination of two optodes, the fluence in the voxels at the optode positions is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluence_at_optodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa5935",
   "metadata": {},
   "source": [
    "## Plot fluence\n",
    "\n",
    "To illustrate the tissue probed by light travelling from a source to the detector two fluence profiles need to be multiplied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bcbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "\n",
    "plt = pv.Plotter()\n",
    "\n",
    "f = fluence_all.loc[\"S1\", 760].values * fluence_all.loc[\"D1\",760].values\n",
    "f[f==0] = f[f!=0].min()\n",
    "f = np.log10(f)\n",
    "vf = pv.wrap(f)\n",
    "\n",
    "plt.add_volume(\n",
    "    vf,\n",
    "    log_scale=False, \n",
    "    cmap='plasma_r',\n",
    "    clim=(-10,0),\n",
    ")\n",
    "cedalion.plots.plot_surface(plt, head.brain, color=\"w\")\n",
    "cedalion.plots.plot_labeled_points(plt, geo3d_snapped_ijk)\n",
    "\n",
    "cog = head.brain.vertices.mean(\"label\").values\n",
    "plt.camera.position = cog + [-300,30, 150]\n",
    "plt.camera.focal_point = cog \n",
    "plt.camera.up = [0,0,1] \n",
    "#plt.reset_camera()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68e889",
   "metadata": {},
   "source": [
    "### Calculate the sensitivity matrices\n",
    "\n",
    "The sensitivity matrix describes the effect of an absorption change at a given surface vertex on the OD recording in a given channel and at given wavelength. The coordinate `is_brain` holds a mask to distinguish brain and scalp voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc196b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Adot = fwm.compute_sensitivity(fluence_all, fluence_at_optodes)\n",
    "Adot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(Adot.shape) * 8 / 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5fe39",
   "metadata": {},
   "source": [
    "The sensitivity `Adot` has shape (nchannel, nvertex, nwavelenghts). To solve the inverse problem we need a matrix that relates OD in channel space to absorption in image space. Hence, the sensitivity must include the extinction coefficients to translate between OD and concentrations. Furthermore, channels at different wavelengths must be  stacked as well vertice and chromophores into new dimensions (flat_channel, flat_vertex):\n",
    "\n",
    "$$ \\left( \\begin{matrix} OD_{c_1, \\lambda_1} \\\\ \\vdots \\\\ OD_{c_N,\\lambda_1} \\\\ OD_{c_1,\\lambda_2} \\\\ \\vdots \\\\ OD_{c_N,\\lambda_2} \\end{matrix}\\right) = A \\cdot\n",
    "\\left( \\begin{matrix} \\Delta c_{v_1, HbO} \\\\ \\vdots \\\\ \\Delta c_{v_N, HbO} \\\\ \\Delta c_{v_1, HbR} \\\\ \\vdots \\\\ \\Delta c_{v_N, HbR} \\end{matrix}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adot_stacked = fwm.compute_stacked_sensitivity(Adot)\n",
    "Adot_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c540707",
   "metadata": {},
   "source": [
    "### Invert the sensitivity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca39270",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = pseudo_inverse_stacked(Adot_stacked)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95e3fa",
   "metadata": {},
   "source": [
    "### Calculate concentration changes\n",
    "\n",
    "- the optical density has shape (nchannel, nwavelength, time) -> stack it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_stacked = od.stack({\"flat_channel\" : [\"wavelength\", \"channel\"]})\n",
    "display(od_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dC = B @ od_stacked\n",
    "dC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb26158",
   "metadata": {},
   "source": [
    "## Plot concentration changes\n",
    "\n",
    "Using functionality from pyvista and VTK plot the concentration changes on the brain and scalp surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = cdc.VTKSurface.from_trimeshsurface(head.brain)\n",
    "b = pv.wrap(b.mesh)\n",
    "s = cdc.VTKSurface.from_trimeshsurface(head.scalp)\n",
    "s = pv.wrap(s.mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0853d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "hbo = dC[:dC.shape[0]//2, :].pint.dequantify()\n",
    "hbo_brain = hbo[(Adot.is_brain == True).values,:]\n",
    "hbo_scalp = hbo[(Adot.is_brain == False).values,:]\n",
    "\n",
    "\n",
    "i = 10\n",
    "tt = od.time[i]\n",
    "b[\"reco_hbo\"] = (hbo_brain[:,i] - hbo_brain[:,0]) / 1e-6 # FIXME unit handling\n",
    "s[\"reco_hbo\"] = (hbo_scalp[:,i] - hbo_scalp[:,0]) / 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d92478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot brain surface\n",
    "plt = pv.Plotter()\n",
    "    \n",
    "plt.add_mesh(\n",
    "    b,\n",
    "    scalars=\"reco_hbo\",\n",
    "    cmap='seismic', # 'gist_earth_r', \n",
    "    clim=(-0.3,0.3),\n",
    "    scalar_bar_args = {\"title\" : \"HbO / µM\"}\n",
    ")\n",
    "\n",
    "cog = head.brain.vertices.mean(\"label\").values\n",
    "plt.camera.position = cog + [0,0,400]\n",
    "plt.camera.focal_point = cog \n",
    "plt.camera.up = [0,1,0] \n",
    "plt.reset_camera()\n",
    "\n",
    "plt.add_text(f\"time: {tt:.3f} s\")\n",
    "cedalion.plots.plot_labeled_points(plt, geo3d_snapped_ijk)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scalp\n",
    "plt = pv.Plotter()\n",
    "    \n",
    "plt.add_mesh(\n",
    "    s,\n",
    "    scalars=\"reco_hbo\",\n",
    "    cmap='seismic', # 'gist_earth_r', \n",
    "    clim=(-.3,.3),\n",
    ")\n",
    "\n",
    "cog = head.brain.vertices.mean(\"label\").values\n",
    "plt.camera.position = cog + [0,0,400]\n",
    "plt.camera.focal_point = cog \n",
    "plt.camera.up = [0,1,0] \n",
    "plt.reset_camera()\n",
    "\n",
    "plt.add_text(f\"time: {tt:.3f} s\")\n",
    "cedalion.plots.plot_labeled_points(plt, geo3d_snapped_ijk)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
